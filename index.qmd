---
title: "Lab 2: Julia Data Handling"
subtitle: "DataFrames.jl, Plotting with CairoMakie, TX precipitation datasets, Basic exploratory data analysis"
author: CEVE 543 Fall 2025
date: "2025-09-05"
type: "lab"
module: 1
week: 2
topics: ["DataFrames", "Plotting", "Precipitation data", "EDA", "AI-assisted coding"]
objectives:
  - "Master DataFrames.jl for data manipulation and analysis"
  - "Create publication-quality plots with CairoMakie.jl"
  - "Perform exploratory data analysis on Texas precipitation datasets"
  - "Use AI copilot effectively for plotting syntax and code assistance"
ps_connection: "Builds data loading, manipulation, and visualization toolkit for PS1 Tasks 1-3"

engine: julia
code-annotations: hover

format:
  html:
    toc: true
    toc-depth: 3
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: svg
  typst:
    fontsize: 11pt
    margin: 
      x: 1in
      y: 1in
    number-sections: true
    fig-format: svg

execute: 
  cache: true
  freeze: auto

# Code formatting options
code-overflow: wrap
code-line-numbers: false
code-block-font-size: "0.85em"
---

## Learning Goals

1. Download and parse real weather data (annual maximum rainfall) from NOAA
1. Create DataFrames to organize station metadata and rainfall measurements
1. Build interactive maps and time series plots
1. Calculate distances between geographic points
1. Compare data across multiple weather stations

## Do Before Lab

1. **Set up GitHub Copilot.** You'll use **GitHub Copilot** - an AI assistant that helps write code. It's especially useful for plotting and data manipulation syntax.
    1. Visit [GitHub Copilot](https://github.com/features/copilot) and sign up using your **education account**
    2. Install the GitHub Copilot extension in VS Code
    3. Link your VS Code to your GitHub account when prompted
    4. You should see Copilot suggestions appear as you type code
1. **Accept the assignment.** Use the GitHub Classroom link posted on Canvas to "accept" the assignment, which will give you your own GitHub repository.
1. **Clone your repository** to your computer using GitHub Desktop or the command line.
1. **Open the repository in VS Code** directly from VS Code, GitHub Desktop, or your command line.
1. **Activate the Julia environment.** In VS Code, open the command palette (View â†’ Command Palette) and type "Julia: Start REPL". Alternatively, run `julia --project=.` in the terminal, then run `using Pkg; Pkg.instantiate()` to install required packages. The easiest method is to type `]` in the REPL to enter package mode, then type `instantiate`.

## Do During Lab

1. **Verify rendering works.** Run `quarto render index.qmd` in your terminal - it should create both PDF and HTML documents.

2. **Work through the lab step by step.** You can either:
   - Run code line by line in your Julia REPL, or
   - Run `quarto preview index.qmd` for a live preview that updates automatically
   
   Make sure you can run each line of code and use the annotations to understand what each line does.

3. **Try these modifications** as you work through the lab:
   - **Change the target location** - Pick a different city and find the nearest weather stations
   - **Adjust the number of stations** - Plot more or fewer stations in the comparison
   - **Modify the map appearance** - Try different color schemes or marker sizes
   - **Explore different time periods** - Focus on specific decades or years

4. **Complete the analysis question** at the end of the document.

## Background

We are deliberately not providing a gentle introduction to Julia here, because everyone is at a different level of familiarity with the language.
Instead, refer to [the course website's Julia learning resources](https://dossgollin-lab.github.io/climate-risk-management-reference/chapters/appendices/julia.html) (*Julia for Nervous Beginners* is a good place to start if that sounds like you!).
You aren't expected to start working on the problem set yet, but you should get up to speed with Julia -- if you have exposure to Julia or are a proficient R/Matlab/Python programmer, this will be a period of light work for you; if you've never seen Julia then it will be heavier work.

**Key Documentation:**

- [TidierData.jl Documentation](https://tidierorg.github.io/Tidier.jl/dev/) - Complete guide to modern data manipulation
- [Makie.jl Plotting](https://docs.makie.org/stable/) - Comprehensive plotting documentation  
- [GeoMakie.jl](https://geo.makie.org/dev/introduction) - Geographic visualization techniques
- [DataFrames.jl](https://dataframes.juliadata.org/stable/) - Core tabular data operations

**Understanding Code Annotations:** This lab uses interactive code annotations to explain what each line does. You'll see numbers like `# <1>` in the code - hover over them to see explanations in popup tooltips.

## Code

### Loading Required Packages

First, we need to load all the Julia packages we'll use in this analysis:

```{julia}
#| output: false
# Core data manipulation packages
using CSV               # <1>
using DataFrames        # <2>
using Dates             # <3>
using Distributions     # <4>
using Downloads         # <5>
using TidierData        # <6>
using TidierFiles       # <7>
using Unitful           # <8>

ENV["DATAFRAMES_ROWS"] = 5 # <8>
```

1. Load CSV package for reading the NOAA text file into Julia
2. Load DataFrames for working with tabular data (like Excel spreadsheets in code)
3. Load Dates to properly parse and handle time information in weather data
4. Load Downloads to fetch files from NOAA's web servers
5. Load TidierData for modern data manipulation with pipeline syntax (`@chain`, `@filter`, `@select`)
6. Load TidierFiles for simplified file reading and writing operations
7. Load Unitful to handle units of measurement (inches, kilometers, degrees) properly
8. Set the maximum number of rows to display in DataFrames by default

Next, we load the plotting packages and configure our visualization backend:

```{julia}
#| output: false
# Plotting and visualization packages
using GLMakie           # <1>
using CairoMakie        # <2>
using ColorSchemes      # <3>
using GeoMakie          # <4>
using Makie.Unitful     # <5>

# Configure plotting backend - CairoMakie for high-quality static plots
CairoMakie.activate!(type = "svg")  # <6>
# If you want interactive plots, uncomment the line below instead:
GLMakie.activate!()  # <7>

# Configure Unitful display preferences to use inches
rainfall_conversion = Makie.UnitfulConversion(u"inch")  # <8>
```

1. Load GLMakie for interactive plots with GPU acceleration (default choice)
2. Load CairoMakie as backup for high-quality static plots if GLMakie fails
3. Load ColorSchemes to access professional color palettes (viridis, plasma, etc.)
4. Load GeoMakie for geographic visualizations and map projections
5. Load Makie.Unitful to integrate our unit system with the plotting functions
6. Activate CairoMakie as the plotting backend for high-quality static plots
7. Alternative activation for interactive plots if needed
8. Create unit converter to display rainfall in inches instead of default "deca-inches"

Finally, we load our custom utility functions that contain the data parsing logic:

```{julia}
#| output: false
# Load custom utility functions
include("util.jl")      # <1>
```

1. Load our custom parsing functions from util.jl (keeps the main notebook clean)

### Downloading Weather Data

Next, we'll download precipitation data from the National Weather Service:

```{julia}
fname = "dur01d_ams_na14v11.txt"   # <1>
url = "https://hdsc.nws.noaa.gov/pub/hdsc/data/tx/dur01d_ams_na14v11.txt"  # <2>

if !isfile(fname)        # <3>
	Downloads.download(url, fname)  # <4>
end
```

1. Set the local filename for our Texas precipitation data
2. NOAA's web address for the Texas Atlas-14 precipitation dataset
3. Only download if we don't already have the file (the `!` means "not")
4. Download the file from NOAA's server to our local directory

This prevents re-downloading the file every time we run the code, which is faster and more polite to NOAA's servers.

### Parsing the Weather Data

Now we'll use our custom function to parse the downloaded data into a useful format:

```{julia}
stations, rainfall_data = read_noaa_data(fname)  # <1>

display(stations)     # <2>
display(rainfall_data) # <3>
```

1. Call our custom function that parses the NOAA file into two DataFrames
2. Display the stations DataFrame (shows first 5 rows due to ENV setting)
3. Display the rainfall DataFrame to see the data structure

The `read_noaa_data` function separates the messy text file into:

- `stations`: A DataFrame with each station's location, name, and metadata
- `rainfall_data`: A DataFrame with all rainfall measurements, using `stnid` to link to stations

### Modern Data Manipulation with TidierData.jl

Now let's explore our data using TidierData.jl, which provides a modern, readable syntax for data manipulation. TidierData brings tidyverse-style operations to Julia with intuitive verb names and pipeline workflows.

For complete documentation, visit: https://tidierorg.github.io/Tidier.jl/dev/

#### Basic TidierData Operations

Let's start with some basic data exploration using TidierData syntax:

```{julia}
@chain stations begin  # <1>
	@select(stnid, noaa_id, name, state, latitude, longitude, years_of_data)  # <2>
end
```

1. Start a pipeline where each operation flows into the next (like %>% in R)
2. Choose specific columns to display (similar to SQL SELECT)

Note that our rainfall measurements have explicit units (you'll see `inch` in the output) thanks to the `Unitful.jl` package.
This makes unit conversions automatic and error-free. For example, to convert to centimeters:

```{julia}
@chain rainfall_data begin
	@mutate(rainfall = uconvert(u"cm", rainfall))  # <1>
end
```

1. Convert rainfall measurements from inches to centimeters automatically with proper unit handling

#### Filtering and Transforming Data

```{julia}
# Find stations in Texas with lots of data
@chain stations begin  # <1>
	@filter(state == "TX" && years_of_data > 50)  # <2>
	@select(noaa_id, name, years_of_data)  # <3>
	@arrange(desc(years_of_data))  # <4>
end
```

1. Start a pipeline using our stations DataFrame
2. Keep only rows where state is "TX" AND years_of_data > 50
3. Choose which columns to show in our result
4. Sort by years_of_data in descending order (most data first)

#### Adding Calculated Columns

```{julia}
# Create categorical data quality labels
@chain stations begin  # <1>
	@mutate(  # <2>
		data_category = case_when(  # <3>
			years_of_data >= 60 => "Long-term (60+ years)",  # <4>
			years_of_data >= 30 => "Medium-term (30-59 years)",  # <5>
			years_of_data >= 10 => "Short-term (10-29 years)",  # <6>
			true => "Very short (<10 years)",  # <7>
		)
	)
	@select(noaa_id, name, years_of_data, data_category)  # <8>
end
```

1. Start our data pipeline with the stations DataFrame
2. Create or modify columns in the DataFrame
3. Use conditional logic to create a new categorical column
4. If 60+ years of data, categorize as "Long-term"
5. If 30-59 years of data, categorize as "Medium-term" 
6. If 10-29 years of data, categorize as "Short-term"
7. Default case for stations with less than 10 years
8. Select only the columns we want to display


#### Grouping and Summarizing Data

```{julia}
# Analyze temporal coverage of rainfall data
obs_per_year = @chain rainfall_data begin  # <1>
	@group_by(year) # <2>
	@summarise(  # <3>
		station_count = n(),  # <4>
	)
	@arrange(desc(station_count))  # <5>
end
```

1. Start with rainfall DataFrame to see which years have the most station coverage
2. Group observations by year to count stations active each year
3. Calculate summary statistics for each year group
4. Count how many stations reported data in each year
5. Sort years by station count (years with most stations first)


### Creating a Map of Weather Stations

Let's visualize all the weather stations on a map, with colors showing how many years of data each station has.

For more advanced mapping techniques, see: https://geo.makie.org/dev/introduction

For complete Makie plotting documentation, visit: https://docs.makie.org/stable/

```{julia}
function plot_stations()

	# Create map plot colored by years of data
	fig = Figure(size = (800, 600))
	ga = GeoAxis(fig[1, 1]; source = "+proj=latlong", dest = "+proj=merc",   # <1>
		title = "Texas Atlas-14 Stations", xgridvisible = false, ygridvisible = false)

	# Add US states (white with black borders)
	states = GeoMakie.naturalearth("admin_1_states_provinces_lakes", 110)  # <2>
	poly!(ga, states.geometry;
		strokecolor = :black, strokewidth = 1, color = :white)

	# Create scatter plot with viridis colormap
	scatter!(ga, stations.longitude, stations.latitude;
		color = stations.years_of_data, colormap = :viridis, markersize = 10)  # <3>

	# Set plot bounds based on data bounds plus delta
	delta = 0.3  # degrees of padding around the data bounds
	min_lon, max_lon = extrema(stations.longitude)
	min_lat, max_lat = extrema(stations.latitude)

	xlims!(ga, min_lon - delta, max_lon + delta)
	ylims!(ga, min_lat - delta, max_lat + delta)

	# Add colorbar
	Colorbar(fig[1, 2], label = "Years of Data", colormap = :viridis,
		limits = (minimum(stations.years_of_data), maximum(stations.years_of_data)))

	return fig
end
plot_stations()
```

1. Create a GeoAxis with map projection: converts lat/lon coordinates to Mercator for proper display
2. Download US state boundaries from Natural Earth (110 = medium resolution) for map background
3. Plot weather stations colored by years of data using the viridis color scheme


### Finding Stations Near a Target Location

Let's say we want to study rainfall at a particular point. We'll use Rice University in Houston as our example:

```{julia}
target_location = "San Antonio"  # <1>  
target_lon =  -98.4839        # <2>
target_lat =  29.5442	      # <3>
```

1. Descriptive name for our target location (change this for different places!)
2. Rice University's longitude coordinate (negative means west of Greenwich)
3. Rice University's latitude coordinate (positive means north of equator)


### Calculating Distances to Weather Stations

Now we need to calculate how far each weather station is from our target location using the Haversine formula:

```{julia}
# Haversine formula for great circle distance with units
function calc_distance(lon1, lat1, lon2, lat2)  # <1>
	R = 6378.0u"km"  # <2>

	# Convert degrees to radians
	Ï†1 = deg2rad(lat1)  # <3>
	Ï†2 = deg2rad(lat2)  # <4>
	Î”Ï† = deg2rad(lat2 - lat1)  # <5>
	Î”Î» = deg2rad(lon2 - lon1)  # <6>

	# Haversine formula
	a = sin(Î”Ï† / 2)^2 + cos(Ï†1) * cos(Ï†2) * sin(Î”Î» / 2)^2  # <7>
	c = 2 * atan(sqrt(a), sqrt(1 - a))  # <8>

	return R * c  # <9>
end

stations = @chain stations begin  # <10>
	@mutate(distance_km = calc_distance(longitude, latitude, !!target_lon, !!target_lat))  # <11>
	@arrange(distance_km)  # <12>
end
stations  # <13>
```

1. Define a function that calculates great circle distance between two lat/lon points
2. Earth's radius in kilometers (using proper units!)
3. Convert the first latitude from degrees to radians (Ï† is Greek phi)
4. Convert the second latitude to radians
5. Calculate difference in latitudes (Î” is Greek delta, means "change in")
6. Calculate difference in longitudes (Î» is Greek lambda)
7. First part of Haversine formula: angular distance component
8. Complete the great circle distance calculation
9. Multiply by Earth's radius to get distance in kilometers
10. Start a TidierData pipeline with our stations DataFrame
11. Add distance column using `!!` to inject external variables into the TidierData expression
12. Sort the stations by distance (closest first)
13. Display the sorted stations (shows closest ones first due to ENV row limit)

The Haversine formula accounts for Earth's curvature, giving much more accurate distances than simple straight-line calculations.

### Examining the Closest Weather Station

Let's focus on the closest weather station and look at its rainfall data:

```{julia}
closest_station = first(stations)
closest_stnid = closest_station.stnid
closest_rainfall = @chain rainfall_data begin
	@filter(stnid == !!closest_stnid)  # <1>
end
closest_rainfall
```

1. Filter rainfall data using `!!` to inject the external variable into TidierData

### Plotting a Time Series

Now let's create a time series plot to see how rainfall has changed over time at this station:

```{julia}
function plot_time_series(station_row, rainfall_df)
	# Create time series plot of rainfall data
	fig = Figure(size = (800, 400))
	ax = Axis(fig[1, 1],
		ylabel = "Annual Maximum Rainfall",
		title = "$(station_row.noaa_id): $(station_row.name)",  # <1>
		dim2_conversion = rainfall_conversion)  # <2>

	# Plot the time series
	lines!(ax, rainfall_df.date, rainfall_df.rainfall)  # <3>
	scatter!(ax, rainfall_df.date, rainfall_df.rainfall,  # <4>
		markersize = 10, marker = :circle, strokewidth = 2, color = :transparent)

	fig
end

plot_time_series(closest_station, closest_rainfall)
```

1. Use string interpolation `$()` to insert station ID and name into the plot title
2. Apply unit converter to display rainfall in inches instead of deca-inches
3. Plot using `date` column to show precise timing of annual maximum events
4. Add hollow circles with transparent fill to emphasize individual data points


### Comparing Multiple Nearby Stations

This is interesting! How does this compare to other nearby stations? Let's plot several stations together:

```{julia}
# Compare with other nearby stations
function plot_nearby_comparison(stations_df, rainfall_data; n_stations = 4)
	fig = Figure(size = (800, 400))
	ax = Axis(fig[1, 1],
		ylabel = "Annual Maximum Rainfall",
		title = "Comparison of $(n_stations) Nearest Stations to $(target_location)",
		dim2_conversion = rainfall_conversion)

	colors = [get(colorschemes[:viridis], i / n_stations) for i in 0:n_stations-1]  # <1>

	for (i, station) in enumerate(eachrow(stations_df[1:n_stations, :]))  # <2>
		station_id = station.stnid
		rainfall = @chain rainfall_data begin
			@filter(stnid == !!station_id)  # <3>
		end
		lines!(ax, rainfall.year, rainfall.rainfall,
			color = colors[i], linewidth = 2,
			label = "$(station.noaa_id)")
		scatter!(ax, rainfall.year, rainfall.rainfall,
			color = colors[i], markersize = 10)
	end
	xlims!(ax, 1950, 2025)

	axislegend(ax, position = :lt)  # <4>
	return fig
end

plot_nearby_comparison(stations, rainfall_data; n_stations = 4)
```

1. Generate evenly-spaced colors from the viridis palette for each station
2. Loop through the first `n_stations` closest weather stations 
3. Use `!!` to inject the loop variable into the TidierData filter expression
4. Add a legend in the top-left corner to identify different stations

### GEV Distribution and Return Period Analysis

Now we'll work with a GEV distribution to create return period plots and compare with observed data using Weibull plotting positions. 

**Your task**: Experiment with different GEV parameters to see how well you can match the observed data from your closest station. You'll need to systematically test each parameter to answer the questions at the end. Start with these parameters, then modify them one at a time:

```{julia}
# GEV parameters for Houston area (units: inches)
# Try modifying these values to better fit your data!
Î¼ = 2.7    # Location parameter - shifts the distribution left/right
Ïƒ = 1.4    # Scale parameter - controls spread (must be > 0)
Î¾ =  0.13   # Shape parameter - controls tail behavior

houston_gev = GeneralizedExtremeValue(Î¼, Ïƒ, Î¾)  # <1>
```

1. Create GEV distribution with the specified parameters

### Return Period Plot

```{julia}
# Function to calculate return levels from GEV
function gev_return_level(gev_dist, return_period)
	Î¼, Ïƒ, Î¾ = params(gev_dist)  # <1>
	if abs(Î¾) < 1e-6  # Gumbel case
		return Î¼ - Ïƒ * log(-log(1 - 1 / return_period))  # <2>
	else
		return Î¼ - (Ïƒ / Î¾) * (1 - (-log(1 - 1 / return_period))^(-Î¾))  # <3>
	end
end

# Create return period plot
function plot_return_periods(gev_dist)
	fig = Figure(size = (800, 400))
	ax = Axis(fig[1, 1],
		xlabel = "Return Period (years)",
		ylabel = "Return Level (inches)",
		title = "GEV Return Period Plot",
		xscale = log10)  # <4>

	# Plot curve and highlight standard return periods
	T_smooth = 10 .^ range(log10(1.1), log10(250), length = 100)
	levels_smooth = [gev_return_level(gev_dist, T) for T in T_smooth]
	lines!(ax, T_smooth, levels_smooth, color = :blue, linewidth = 2, label = "GEV Model")

	return_periods = [5, 10, 25, 50, 100, 250]  # <5>

	ax.xticks = [5, 10, 25, 50, 100, 250]
	return fig
end

plot_return_periods(houston_gev)
```

1. Extract GEV parameters from distribution object
2. Gumbel formula when $\xi \approx 0$
3. General GEV return level formula
4. Log scale for x-axis
5. Standard return periods for flood analysis

### Weibull Plotting Positions - Comparing Theory vs Reality

Now let's see how well your chosen GEV parameters match the actual observed data! The red dots show the empirical return periods calculated from your station's data using Weibull plotting positions.

**What to look for as you experiment:**
- Do the red dots follow the blue GEV curve? 
- Are there systematic deviations at high or low return periods?
- How does each parameter change affect the fit?

**Systematic experimentation approach:**
1. First, try increasing $\mu$ by 1-2 inches, then decreasing it. Note the effect.
2. Next, try increasing $\sigma$ by 0.5, then decreasing it. Note the effect.
3. Finally, try changing $\xi$ to 0.0 (Gumbel), then to -0.1 (light tail), then 0.3 (heavy tail). Note the effect.
4. Find your best combination!

```{julia}
# Weibull plotting positions
function weibull_plotting_positions(data)
	clean_data = sort(collect(skipmissing(data)))  # <1>
	n = length(clean_data)
	plotting_positions = [i / (n + 1) for i in 1:n]  # <2>
	empirical_return_periods = [1 / (1 - p) for p in plotting_positions]  # <3>
	return clean_data, empirical_return_periods
end

# Plot empirical vs theoretical
function plot_empirical_vs_theoretical(station_data, gev_dist, station_info)
	# Start with the basic return period plot
	fig = plot_return_periods(gev_dist)  # <4>
	ax = fig.content[1]  # Get the actual Axis object

	# Update title to include station info
	ax.title = "Empirical vs Theoretical Return Periods\n$(station_info.noaa_id): $(station_info.name)"  # <5>

	# Add empirical data with Weibull positions
	emp_levels, emp_periods = weibull_plotting_positions(station_data.rainfall)
	scatter!(ax, emp_periods, emp_levels,
		color = :red, markersize = 10,
		label = "Observed Data (Weibull positions)")  # <6>

	axislegend(ax, position = :rb)

	return fig
end

plot_empirical_vs_theoretical(closest_rainfall, houston_gev, closest_station)
```

## Analysis Questions

### Temporal Trends

Explore the rainfall data to find one station where there *appears* to be a positive trend in rainfall over time and one station where there *appears* to be a negative trend. We will discuss more formal ways for thinking about trends in the coming weeks.

**Station with apparent positive trend:**

*Write your answer here. Include the station name/ID and explain what you observe in the data that suggests an increasing trend.*

While there is around 50 years missing from this station, I would say out of all the stations I checked, this most resembles a positive trend. Station 41-4427, or Dallas WB, resembles a positive trend because the data points in the earlier years are generally lower, and the more recent years show higher annual maximum rainfall values. This upward pattern over time indicates a potential positive trend. 

**Station with apparent negative trend:**

*Write your answer here. Include the station name/ID and explain what you observe in the data that suggests a decreasing trend.*

Unfortunately I could not find a station with a clear negative trend. The closest I found was the San Antonio INTL AP (79-0045) station where the annual maximums of rainfall were slightly decreasing. The maximum values of rainfall were generally higher in early years than recent years which indicate a possible negative trend.

### GEV Parameter Experimentation

**Starting Parameters vs Final Parameters:**

*Record your starting parameters and the final parameters you chose after experimentation:*

- Starting: $\mu = 4.0$, $\sigma = 1.3$, $\xi = 0.15$
- Final: $\mu = 2.7$ $\sigma = 1.4$, $\xi = 0.13$

**Parameter Effects:**

**What happened when you increased/decreased $\mu$ (location parameter)?**

*Test values like $\mu = 2.0, 4.0, 6.0$. Describe how the entire GEV curve shifted and how this affected the fit to your red data points.*

Decreasing the $\mu$ changes it so that the curve moves up or down. When I increase $\mu$ , the curve shifts upwards and when I decrease it, the curve shifts downwards. I tried to fit it so that most of the data points fit along the curve. 

**What happened when you increased/decreased $\sigma$ (scale parameter)?**

*Test values like $\sigma = 0.8, 1.3, 2.0$. Describe how the curve became steeper/flatter and how this affected the spread of return levels.*

Decreasing $\sigma$ made the curve flatter, causing the return levels to be closer together, while increasing $\sigma$ made the curve steeper and the spread of return levels wider apart. 

**What happened when you changed $\xi$ (shape parameter)?** 

*Test $\xi = -0.1$ (light tail), $\xi = 0.0$ (Gumbel), $\xi = 0.15$, $\xi = 0.3$ (heavy tail). Focus on how the high return period end (right side) of the curve changed.*

Decreasing $\xi$ to make it negative made the tail of the curve downwards, while a value of $\xi = 0$ created a straight tail and a higher value made the curve bend upwards, indicating a heavier tail and higher probability of extreme events. 

**Final Assessment:**

**How well does your best-fit GEV distribution represent the closest station's data?**

*Do the red dots follow your blue curve? Where are the biggest deviations?*

I think the curve matched the dots pretty well except the return period between 10 and 25 years where the observed data has a dip. The GEV distribution was not able to capture local extremes but I think it captured the overall trend pretty well!


**Additional observations:**

*Optional: Add any other interesting patterns you notice in the return period analysis.*




